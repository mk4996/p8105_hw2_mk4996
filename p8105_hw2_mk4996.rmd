---
title: "p8105_hw2_mk4996"
author: "Miho Kawanami"
date: "2025-09-27"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
options(scipen = 999) 
```
# Problem 1

## First: Cleaning the data in pols-month.csv.
```{r}
pols_df = read_csv(file = "./pols-month.csv", 
                          na = c(".", "NA", ""), 
                          show_col_types = FALSE) |>
  clean_names() |>
  separate(mon, into = c("year", "month", "day"), sep = "-", convert = TRUE) |>
  mutate(
    month = factor(month.name[month], levels = month.name, ordered = TRUE),
    president = case_when(
      prez_gop %in% c(1, 2) ~ "gop",
      prez_dem == 1 ~ "dem",
      TRUE ~ NA_character_
    )) |>
  select(-day, -prez_dem, -prez_gop)
```

```{r, include=FALSE}
head(pols_df)
```

## Second: Cleaning the data in snp.csv.
```{r}
snp_df = read_csv(file = "./snp.csv",
                         na = c(".", "NA", ""),
                         show_col_types = FALSE) |>
  clean_names() |>
  separate(date, into = c("month", "day", "year"), sep = "/", convert = TRUE) |>
  mutate(
  year = ifelse(year < 20, year + 2000, year + 1900),  
  month = factor(month.name[month], levels = month.name, ordered = TRUE)) |>
  select(year, month, close) |>
  arrange(year, month)
```

```{r, include=FALSE}
  head(snp_df)
```

## Third: Tidying the unemployment data
```{r}
unemp_df = read_csv(file = "./unemployment.csv",
                         na = c(".", "NA", ""),
                         show_col_types = FALSE) |>
  clean_names() |>
  pivot_longer(-year, names_to = "month", values_to = "unemployment") |>
  mutate(
    month = match(tolower(month), tolower(month.abb)),
    month = factor(month.name[month], levels = month.name, ordered = TRUE)
  ) |>
  arrange(year, month)
```

```{r, include=FALSE}
  head(unemp_df)
```

## Join the datasets
```{r}
pols_snp  = left_join(pols_df, snp_df,  by = c("year","month"))
final_df  = left_join(pols_snp, unemp_df, by = c("year","month")) |>
  arrange(year, month)
```

```{r, include=FALSE}
  head(final_df)
```

## Write a short paragraph about these datasets

The `pols-month` dataset contained the number of national politicians who are democratic or republican at any given time.
The `snp` dataset contained Standard & Poor’s stock market index (S&P), often used as a representative measure of stock market as a whole. 
The `unemployment` dataset reports monthly unemployment percentages.  

After merging, the resulting dataset has `r nrow(final_df)` rows and `r ncol(final_df)` columns, ranging years `r min(final_df$year)` to `r max(final_df$year)`.  
Key variables include `year`, `month`, `president`, `close` (the closing values of the S&P stock index), and `unemployment`.

# Problem2
## Cleaning the data in Mr. Trash Wheel
```{r}
library(readxl)
mr_trash = 
  read_excel("./202509 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N710") |> 
  clean_names() |> 
  drop_na(dumpster) |> 
  mutate(year = parse_integer(as.character(year)),
        (sports_balls = as.integer(round(sports_balls))),
        (trash_wheel = "Mr. Trash Wheel")) 
```

```{r, include=FALSE}
head(mr_trash)
```

## Cleaning the data in Professor Trash Wheel
```{r}
prof_trash = 
  read_excel("./202509 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:M135") |> 
  clean_names() |> 
  drop_na(dumpster) |> 
  mutate(trash_wheel = "Professor Trash Wheel")
```

```{r, include=FALSE}
head(prof_trash)
```

## Cleaning the data in Gwynns Falls Trash Wheel
```{r}
gwynnda = 
  read_excel("./202509 Trash Wheel Collection Data.xlsx", sheet = "Gwynns Falls Trash Wheel", range = "A2:L352") |> 
  clean_names() |> 
  drop_na(dumpster) |> 
  mutate(trash_wheel = "Gwynns Falls Trash Wheel")
```

```{r, include=FALSE}
head(gwynnda)
```

## Combine the datasets
```{r}
trash_data = bind_rows(mr_trash, prof_trash, gwynnda)
```

```{r, include=FALSE}
head(trash_data)
```

## Write a short paragraph about these datasets
Mr. Trash Wheel is a water-wheel vessel that intercepts litter and debris in Baltimore’s Inner Harbor; the public spreadsheet records dumpster-level collections, including dates, weights/volumes, and counts of common items. Using those sheets, I combined the Mr., Professor, and Gwynnda data into a single tidy dataset with `r nrow(trash_data)` observations across `r n_distinct(trash_data$trash_wheel)` devices. Each row corresponds to one dumpster and includes fields such as `dumpster`, `date`, `year`, `month`, `weight_tons`, `volume_cubic_yards`, and item counts (e.g., `plastic_bottles`, `polystyrene`, `cigarette_butts`, `glass_bottles`, `plastic_bags`, `wrappers`; `sports_balls` is recorded only on some sheets). For the available data, **Professor Trash Wheel** collected a total of `r trash_data |> filter(trash_wheel == "Professor Trash Wheel") |> summarise(total = sum(weight_tons, na.rm = TRUE)) |> pull(total) |> round(1)` tons of trash, and **Gwynnda** collected `r trash_data |> filter(trash_wheel == "Gwynns Falls Trash Wheel", year == 2022, str_detect(tolower(month), "june")) |> summarise(total = sum(cigarette_butts, na.rm = TRUE)) |> pull(total)` cigarette butts in June 2022.


# Problem 3
## Cleaning the data in Zip Codes
```{r}
library(janitor)
zip_codes = read_csv("./Zip Codes.csv", na = c(".", "NA", ""), show_col_types = FALSE) |>
  clean_names() |>
  mutate(
    zip = as.character(zip_code),  
    borough = recode(
      county,
      "New York" = "Manhattan",
      "Kings"    = "Brooklyn",
      "Queens"   = "Queens",
      "Bronx"    = "Bronx",
      "Richmond" = "Staten Island"
  )) |>
select(zip, neighborhood, borough) |>
  group_by(zip) |>
  summarise(
    borough      = str_c(sort(unique(na.omit(borough))), collapse = " / "),
    neighborhood = str_c(sort(unique(na.omit(neighborhood))), collapse = ", "),
    .groups = "drop"
  ) |>
 mutate(
    neighborhood = na_if(neighborhood, "") 
  )
```

## Cleaning the data in Zip zori
```{r}
zori_wide = read_csv("./Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv", show_col_types = FALSE) |>
  clean_names() |>
  rename(zip = region_name) |>
  mutate(zip = sprintf("%05s", gsub("\\D", "", as.character(zip))))

date_cols = names(zori_wide)[grepl("^x\\d{4}_\\d{2}_\\d{2}$", names(zori_wide))]

zori_long = zori_wide |>
  pivot_longer(
    cols = all_of(date_cols),
    names_to = "date",
    values_to = "zori"
  ) |>
  mutate(date = as.Date(gsub("^x", "", gsub("_", "-", date)), format = "%Y-%m-%d")) |>
  arrange(zip, date)
```

## Join the data in Zip Codes and in Zip zori
```{r}
full = zori_long |>
  left_join(zip_codes, by = "zip", relationship = "many-to-one")

```

```{r, include=FALSE}
glimpse(full)
```

## Describe the resulting tidy dataset
```{r p3-summary, message=FALSE, warning=FALSE}
n_obs   = nrow(full)
n_zip   = n_distinct(full$zip)
n_neigh = n_distinct(full$neighborhood, na.rm = TRUE)
d_min   = min(full$date, na.rm = TRUE)
d_max   = max(full$date, na.rm = TRUE)

```
The tidy Zillow dataset contains `r n_obs` observations across `r n_zip` ZIP codes and `r n_neigh` neighborhoods, with monthly ZORI values from `r format(d_min, "%b %Y")` to `r format(d_max, "%b %Y")`. Key variables are `zip`, `borough`, `neighborhood`, `date`, and `zori`.

## ZIP codes appear in the ZIP code dataset but not in the Zillow dataset
```{r p3-absent, message=FALSE, warning=FALSE}

zips_in_zori = zori_wide |> distinct(zip)

absent_zips = zip_codes |>
  anti_join(zips_in_zori, by = "zip") |>
  arrange(borough, zip)

absent_n = nrow(absent_zips)

knitr::kable(
  absent_zips |> slice_head(n = 10),
  caption = "ZIPs present in ZIP dictionary but absent from Zillow ZORI (first 10)"
)
```

There are `r absent_n` ZIP codes in the ZIP dictionary that do not appear in the ZORI dataset. Likely reasons include PO-box–oriented ZIPs, areas with very limited rental markets, boundary/coding differences across sources, or retired/new ZIPs not covered in this release.


##  10 ZIP codes (along with the borough and neighborhood) with largest drop in price from January 2020
```{r p3-drops, message=FALSE, warning=FALSE}

d2020 = as.Date("2020-01-31"); if (!any(full$date == d2020)) d2020 = as.Date("2020-01-01")
d2021 = as.Date("2021-01-31"); if (!any(full$date == d2021)) d2021 = as.Date("2021-01-01")

drops <-
  full |>
  filter(date %in% c(d2020, d2021)) |>
  select(zip, borough, neighborhood, date, zori) |>
  pivot_wider(
    names_from  = date,
    values_from = zori,
    names_glue  = "zori_{format(date, '%Y_%m')}"
  ) |>
  filter(!is.na(zori_2020_01), !is.na(zori_2021_01)) |>
  mutate(
    change     = zori_2021_01 - zori_2020_01,
    pct_change = change / zori_2020_01
  ) |>
  arrange(change) |>
  slice_head(n = 10)

knitr::kable(
  drops |>
    transmute(
      ZIP = zip,
      Borough = borough,
      Neighborhood = neighborhood,
      `Jan 2020` = round(zori_2020_01, 0),
      `Jan 2021` = round(zori_2021_01, 0),
      Change = round(change, 0),
      `Pct change` = scales::percent(pct_change, accuracy = 0.1)
    ),
  caption = "Largest drops in ZORI: Jan 2020 → Jan 2021 (Top 10)"
)
```

**Comment.** 
The largest declines are concentrated in high-rent neighborhoods, consistent with the temporary softening of urban rental demand during 2020–2021.

All top-10 ZIP codes with the largest Jan-2020 → Jan-2021 ZORI declines are in **Manhattan** (e.g., Lower Manhattan, Lower East Side, Chelsea/Clinton, Greenwich Village/Soho). The median percent drop among these ZIPs is `r scales::percent(median(drops$pct_change), accuracy = 0.1)` (range `r paste(scales::percent(min(drops$pct_change), 0.1), "to", scales::percent(max(drops$pct_change), 0.1))`). This concentration in high-rent neighborhoods is consistent with a temporary softening of urban rental demand during 2020–2021. 
*Note:* ZIP `r paste(drops$zip[is.na(drops$neighborhood)], collapse = ", ")` lacks a neighborhood label in the ZIP dictionary.

